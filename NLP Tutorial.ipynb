{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Tutorial - NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1. Exploration](#exploration)<br>\n",
    "[2. Tokenization](#tokenization)<br>\n",
    "[3. Stemming](#stemming)<br>\n",
    "[4. Lemmatization](#lemmatization)<br>\n",
    "[5. Stopwords](#stopwords)<br>\n",
    "[6. Chunking](#chunking)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explortion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cmudict', 'cmudict.zip', 'gazetteers', 'gazetteers.zip', 'genesis', 'genesis.zip', 'gutenberg', 'gutenberg.zip', 'inaugural', 'inaugural.zip', 'movie_reviews', 'movie_reviews.zip', 'names', 'names.zip', 'omw-1.4.zip', 'omw.zip', 'shakespeare', 'shakespeare.zip', 'stopwords', 'stopwords.zip', 'treebank', 'treebank.zip', 'twitter_samples', 'twitter_samples.zip', 'wordnet.zip', 'wordnet2021.zip', 'wordnet31.zip', 'wordnet_ic', 'wordnet_ic.zip', 'words', 'words.zip']\n"
     ]
    }
   ],
   "source": [
    "# Review corpora from nltk library\n",
    "print(os.listdir(nltk.data.find(\"corpora\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fellow', '-', 'Citizens', 'of', 'the', 'Senate', ...]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import inaugural\n",
    "inaugural.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152901"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inaugural.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "spch_words = inaugural.words()[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fellow - Citizens of the Senate and of the House of Representatives : Among the vicissitudes incident to life no event could have filled me with greater anxieties than that of which the notification was transmitted by your order , and received on the 14th day of the present month . On the one hand , I was summoned by my Country , whose voice I can never hear but with veneration and love , from a retreat which I had chosen with the fondest predilection , and , in my flattering hopes , with an immutable decision , as the asylum of my declining years -- a retreat which was rendered every day more necessary as well as more dear to me by the addition of habit to inclination , and of frequent interruptions in my health to the gradual waste committed on it by time . On the other hand , the magnitude and difficulty of the trust to which the voice of my country called me , being sufficient to awaken in the wisest and most experienced of her citizens a distrustful scrutiny into his qualifications , could not but overwhelm with despondence one who ( inheriting inferior endowments from nature and unpracticed in the duties of civil administration ) ought to be peculiarly conscious of his own deficiencies . In this conflict of emotions all I dare aver is that it has been my faithful study to collect my duty from a just appreciation of every circumstance by which it might be affected . All I dare hope is that if , in executing this task , I have been too much swayed by a grateful remembrance of former instances , or by an affectionate sensibility to this transcendent proof of the confidence of my fellow citizens , and have thence too little consulted my incapacity as well as disinclination for the weighty and untried cares before me , my error will be palliated by the motives which mislead me , and its consequences be judged by my country with some share of the partiality in which they originated . Such being the impressions under which I have , in obedience to the public summons , repaired to the present station , it would be peculiarly improper to omit in this first official act my fervent supplications to that Almighty Being who rules over the universe , who presides in the councils of nations , and whose providential aids can supply every human defect , that His benediction may consecrate to the liberties and happiness of the people of the United States a Government instituted by themselves for these essential purposes , and may enable every instrument employed in its administration to execute with success the functions allotted to his charge . In tendering this homage to the Great Author of every public and private good , I assure myself that it expresses your sentiments not less than my own , nor those of my fellow "
     ]
    }
   ],
   "source": [
    "for word in spch_words[:500]:\n",
    "    print(word, sep=' ', end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "spch = ' '.join(list(spch_words))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If we wanted to perform an analysis on just one txt file we could do so using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-Washington.txt',\n",
       " '1793-Washington.txt',\n",
       " '1797-Adams.txt',\n",
       " '1801-Jefferson.txt',\n",
       " '1805-Jefferson.txt',\n",
       " '1809-Madison.txt',\n",
       " '1813-Madison.txt',\n",
       " '1817-Monroe.txt',\n",
       " '1821-Monroe.txt',\n",
       " '1825-Adams.txt',\n",
       " '1829-Jackson.txt',\n",
       " '1833-Jackson.txt',\n",
       " '1837-VanBuren.txt',\n",
       " '1841-Harrison.txt',\n",
       " '1845-Polk.txt',\n",
       " '1849-Taylor.txt',\n",
       " '1853-Pierce.txt',\n",
       " '1857-Buchanan.txt',\n",
       " '1861-Lincoln.txt',\n",
       " '1865-Lincoln.txt',\n",
       " '1869-Grant.txt',\n",
       " '1873-Grant.txt',\n",
       " '1877-Hayes.txt',\n",
       " '1881-Garfield.txt',\n",
       " '1885-Cleveland.txt',\n",
       " '1889-Harrison.txt',\n",
       " '1893-Cleveland.txt',\n",
       " '1897-McKinley.txt',\n",
       " '1901-McKinley.txt',\n",
       " '1905-Roosevelt.txt',\n",
       " '1909-Taft.txt',\n",
       " '1913-Wilson.txt',\n",
       " '1917-Wilson.txt',\n",
       " '1921-Harding.txt',\n",
       " '1925-Coolidge.txt',\n",
       " '1929-Hoover.txt',\n",
       " '1933-Roosevelt.txt',\n",
       " '1937-Roosevelt.txt',\n",
       " '1941-Roosevelt.txt',\n",
       " '1945-Roosevelt.txt',\n",
       " '1949-Truman.txt',\n",
       " '1953-Eisenhower.txt',\n",
       " '1957-Eisenhower.txt',\n",
       " '1961-Kennedy.txt',\n",
       " '1965-Johnson.txt',\n",
       " '1969-Nixon.txt',\n",
       " '1973-Nixon.txt',\n",
       " '1977-Carter.txt',\n",
       " '1981-Reagan.txt',\n",
       " '1985-Reagan.txt',\n",
       " '1989-Bush.txt',\n",
       " '1993-Clinton.txt',\n",
       " '1997-Clinton.txt',\n",
       " '2001-Bush.txt',\n",
       " '2005-Bush.txt',\n",
       " '2009-Obama.txt',\n",
       " '2013-Obama.txt',\n",
       " '2017-Trump.txt',\n",
       " '2021-Biden.txt']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.inaugural.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chief', 'Justice', 'Roberts', ',', 'President', ...]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "spch_data = nltk.corpus.inaugural.words('2017-Trump.txt')\n",
    "spch_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Poems by William Blake 1789 ] SONGS OF INNOCENCE AND OF EXPERIENCE and THE BOOK of THEL SONGS OF INNOCENCE INTRODUCTION Piping down the valleys wild , Piping songs of pleasant glee , On a cloud I saw a child , And he laughing said to me : \" Pipe a song about a Lamb !\" So I piped with merry cheer . \" Piper , pipe that song again ;\" So I piped : he wept to hear . \" Drop thy pipe , thy happy pipe ; Sing thy songs of happy cheer :!\" So I sang the same again , While he wept with joy to hear . \" Piper , sit thee down and write In a book , that all may read .\" So he vanish ' d from my sight ; And I pluck ' d a hollow reed , And I made a rural pen , And I stain ' d the water clear , And I wrote my happy songs Every child may joy to hear . THE SHEPHERD How sweet is the Shepherd ' s sweet lot ! From the morn to the evening he stays ; He shall follow his sheep all the day , And his tongue shall be filled with praise . For he hears the lambs ' innocent call , And he hears the ewes ' tender reply ; He is watching while they are in peace , For they know when their Shepherd is nigh . THE ECHOING GREEN The sun does arise , And make happy the skies ; The merry bells ring To welcome the Spring ; The skylark and thrush , The birds of the bush , Sing louder around To the bells ' cheerful sound ; While our sports shall be seen On the echoing Green . Old John , with white hair , Does laugh away care , Sitting under the oak , Among the old folk . They laugh at our play , And soon they all say , \" Such , such were the joys When we all -- girls and boys -- In our youth - time were seen On the echoing Green .\" Till the little ones , weary , No more can be merry : The sun does descend , And our sports have an end . Round the laps of their mothers Many sisters and brothers , Like birds in their nest , Are ready for rest , And sport no more seen On the darkening green . THE LAMB Little Lamb , who make thee Dost thou know who made thee , Gave thee life , and bid thee feed By the stream and o ' er the mead ; Gave thee clothing of delight , Softest clothing , wolly , bright ; Gave thee such a tender voice , Making all the vales rejoice ? Little Lamb , who made thee ? Dost thou know who made thee ? Little Lamb , "
     ]
    }
   ],
   "source": [
    "#view first 500 words\n",
    "for word in mov_data[:500]:\n",
    "    print(word, sep=' ', end=' ')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fellow',\n",
       " '-',\n",
       " 'Citizens',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Senate',\n",
       " 'and',\n",
       " 'of',\n",
       " 'the',\n",
       " 'House',\n",
       " 'of',\n",
       " 'Representatives',\n",
       " ':',\n",
       " 'Among',\n",
       " 'the',\n",
       " 'vicissitudes',\n",
       " 'incident',\n",
       " 'to',\n",
       " 'life',\n",
       " 'no',\n",
       " 'event',\n",
       " 'could',\n",
       " 'have',\n",
       " 'filled',\n",
       " 'me',\n",
       " 'with',\n",
       " 'greater',\n",
       " 'anxieties',\n",
       " 'than',\n",
       " 'that',\n",
       " 'of',\n",
       " 'which',\n",
       " 'the',\n",
       " 'notification',\n",
       " 'was',\n",
       " 'transmitted',\n",
       " 'by',\n",
       " 'your',\n",
       " 'order',\n",
       " ',',\n",
       " 'and',\n",
       " 'received',\n",
       " 'on',\n",
       " 'the',\n",
       " '14th',\n",
       " 'day',\n",
       " 'of',\n",
       " 'the',\n",
       " 'present',\n",
       " 'month',\n",
       " '.',\n",
       " 'On',\n",
       " 'the',\n",
       " 'one',\n",
       " 'hand',\n",
       " ',',\n",
       " 'I',\n",
       " 'was',\n",
       " 'summoned',\n",
       " 'by',\n",
       " 'my',\n",
       " 'Country',\n",
       " ',',\n",
       " 'whose',\n",
       " 'voice',\n",
       " 'I',\n",
       " 'can',\n",
       " 'never',\n",
       " 'hear',\n",
       " 'but',\n",
       " 'with',\n",
       " 'veneration',\n",
       " 'and',\n",
       " 'love',\n",
       " ',',\n",
       " 'from',\n",
       " 'a',\n",
       " 'retreat',\n",
       " 'which',\n",
       " 'I',\n",
       " 'had',\n",
       " 'chosen',\n",
       " 'with',\n",
       " 'the',\n",
       " 'fondest',\n",
       " 'predilection',\n",
       " ',',\n",
       " 'and',\n",
       " ',',\n",
       " 'in',\n",
       " 'my',\n",
       " 'flattering',\n",
       " 'hopes',\n",
       " ',',\n",
       " 'with',\n",
       " 'an',\n",
       " 'immutable',\n",
       " 'decision',\n",
       " ',',\n",
       " 'as',\n",
       " 'the',\n",
       " 'asylum',\n",
       " 'of',\n",
       " 'my',\n",
       " 'declining',\n",
       " 'years',\n",
       " '--',\n",
       " 'a',\n",
       " 'retreat',\n",
       " 'which',\n",
       " 'was',\n",
       " 'rendered',\n",
       " 'every',\n",
       " 'day',\n",
       " 'more',\n",
       " 'necessary',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'more',\n",
       " 'dear',\n",
       " 'to',\n",
       " 'me',\n",
       " 'by',\n",
       " 'the',\n",
       " 'addition',\n",
       " 'of',\n",
       " 'habit',\n",
       " 'to',\n",
       " 'inclination',\n",
       " ',',\n",
       " 'and',\n",
       " 'of',\n",
       " 'frequent',\n",
       " 'interruptions',\n",
       " 'in',\n",
       " 'my',\n",
       " 'health',\n",
       " 'to',\n",
       " 'the',\n",
       " 'gradual',\n",
       " 'waste',\n",
       " 'committed',\n",
       " 'on',\n",
       " 'it',\n",
       " 'by',\n",
       " 'time',\n",
       " '.',\n",
       " 'On',\n",
       " 'the',\n",
       " 'other',\n",
       " 'hand',\n",
       " ',',\n",
       " 'the',\n",
       " 'magnitude',\n",
       " 'and',\n",
       " 'difficulty',\n",
       " 'of',\n",
       " 'the',\n",
       " 'trust',\n",
       " 'to',\n",
       " 'which',\n",
       " 'the',\n",
       " 'voice',\n",
       " 'of',\n",
       " 'my',\n",
       " 'country',\n",
       " 'called',\n",
       " 'me',\n",
       " ',',\n",
       " 'being',\n",
       " 'sufficient',\n",
       " 'to',\n",
       " 'awaken',\n",
       " 'in',\n",
       " 'the',\n",
       " 'wisest',\n",
       " 'and',\n",
       " 'most',\n",
       " 'experienced',\n",
       " 'of',\n",
       " 'her',\n",
       " 'citizens',\n",
       " 'a',\n",
       " 'distrustful',\n",
       " 'scrutiny',\n",
       " 'into',\n",
       " 'his',\n",
       " 'qualifications',\n",
       " ',',\n",
       " 'could',\n",
       " 'not',\n",
       " 'but',\n",
       " 'overwhelm',\n",
       " 'with',\n",
       " 'despondence',\n",
       " 'one',\n",
       " 'who',\n",
       " '(',\n",
       " 'inheriting',\n",
       " 'inferior',\n",
       " 'endowments',\n",
       " 'from',\n",
       " 'nature',\n",
       " 'and',\n",
       " 'unpracticed',\n",
       " 'in',\n",
       " 'the',\n",
       " 'duties',\n",
       " 'of',\n",
       " 'civil',\n",
       " 'administration',\n",
       " ')',\n",
       " 'ought',\n",
       " 'to',\n",
       " 'be',\n",
       " 'peculiarly',\n",
       " 'conscious',\n",
       " 'of',\n",
       " 'his',\n",
       " 'own',\n",
       " 'deficiencies',\n",
       " '.',\n",
       " 'In',\n",
       " 'this',\n",
       " 'conflict',\n",
       " 'of',\n",
       " 'emotions',\n",
       " 'all',\n",
       " 'I',\n",
       " 'dare',\n",
       " 'aver',\n",
       " 'is',\n",
       " 'that',\n",
       " 'it',\n",
       " 'has',\n",
       " 'been',\n",
       " 'my',\n",
       " 'faithful',\n",
       " 'study',\n",
       " 'to',\n",
       " 'collect',\n",
       " 'my',\n",
       " 'duty',\n",
       " 'from',\n",
       " 'a',\n",
       " 'just',\n",
       " 'appreciation',\n",
       " 'of',\n",
       " 'every',\n",
       " 'circumstance',\n",
       " 'by',\n",
       " 'which',\n",
       " 'it',\n",
       " 'might',\n",
       " 'be',\n",
       " 'affected',\n",
       " '.',\n",
       " 'All',\n",
       " 'I',\n",
       " 'dare',\n",
       " 'hope',\n",
       " 'is',\n",
       " 'that',\n",
       " 'if',\n",
       " ',',\n",
       " 'in',\n",
       " 'executing',\n",
       " 'this',\n",
       " 'task',\n",
       " ',',\n",
       " 'I',\n",
       " 'have',\n",
       " 'been',\n",
       " 'too',\n",
       " 'much',\n",
       " 'swayed',\n",
       " 'by',\n",
       " 'a',\n",
       " 'grateful',\n",
       " 'remembrance',\n",
       " 'of',\n",
       " 'former',\n",
       " 'instances',\n",
       " ',',\n",
       " 'or',\n",
       " 'by',\n",
       " 'an',\n",
       " 'affectionate',\n",
       " 'sensibility',\n",
       " 'to',\n",
       " 'this',\n",
       " 'transcendent',\n",
       " 'proof',\n",
       " 'of',\n",
       " 'the',\n",
       " 'confidence',\n",
       " 'of',\n",
       " 'my',\n",
       " 'fellow',\n",
       " 'citizens',\n",
       " ',',\n",
       " 'and',\n",
       " 'have',\n",
       " 'thence',\n",
       " 'too',\n",
       " 'little',\n",
       " 'consulted',\n",
       " 'my',\n",
       " 'incapacity',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'disinclination',\n",
       " 'for',\n",
       " 'the',\n",
       " 'weighty',\n",
       " 'and',\n",
       " 'untried',\n",
       " 'cares',\n",
       " 'before',\n",
       " 'me',\n",
       " ',',\n",
       " 'my',\n",
       " 'error',\n",
       " 'will',\n",
       " 'be',\n",
       " 'palliated',\n",
       " 'by',\n",
       " 'the',\n",
       " 'motives',\n",
       " 'which',\n",
       " 'mislead',\n",
       " 'me',\n",
       " ',',\n",
       " 'and',\n",
       " 'its',\n",
       " 'consequences',\n",
       " 'be',\n",
       " 'judged',\n",
       " 'by',\n",
       " 'my',\n",
       " 'country',\n",
       " 'with',\n",
       " 'some',\n",
       " 'share',\n",
       " 'of',\n",
       " 'the',\n",
       " 'partiality',\n",
       " 'in',\n",
       " 'which',\n",
       " 'they',\n",
       " 'originated',\n",
       " '.',\n",
       " 'Such',\n",
       " 'being',\n",
       " 'the',\n",
       " 'impressions',\n",
       " 'under',\n",
       " 'which',\n",
       " 'I',\n",
       " 'have',\n",
       " ',',\n",
       " 'in',\n",
       " 'obedience',\n",
       " 'to',\n",
       " 'the',\n",
       " 'public',\n",
       " 'summons',\n",
       " ',',\n",
       " 'repaired',\n",
       " 'to',\n",
       " 'the',\n",
       " 'present',\n",
       " 'station',\n",
       " ',',\n",
       " 'it',\n",
       " 'would',\n",
       " 'be',\n",
       " 'peculiarly',\n",
       " 'improper',\n",
       " 'to',\n",
       " 'omit',\n",
       " 'in',\n",
       " 'this',\n",
       " 'first',\n",
       " 'official',\n",
       " 'act',\n",
       " 'my',\n",
       " 'fervent',\n",
       " 'supplications',\n",
       " 'to',\n",
       " 'that',\n",
       " 'Almighty',\n",
       " 'Being',\n",
       " 'who',\n",
       " 'rules',\n",
       " 'over',\n",
       " 'the',\n",
       " 'universe',\n",
       " ',',\n",
       " 'who',\n",
       " 'presides',\n",
       " 'in',\n",
       " 'the',\n",
       " 'councils',\n",
       " 'of',\n",
       " 'nations',\n",
       " ',',\n",
       " 'and',\n",
       " 'whose',\n",
       " 'providential',\n",
       " 'aids',\n",
       " 'can',\n",
       " 'supply',\n",
       " 'every',\n",
       " 'human',\n",
       " 'defect',\n",
       " ',',\n",
       " 'that',\n",
       " 'His',\n",
       " 'benediction',\n",
       " 'may',\n",
       " 'consecrate',\n",
       " 'to',\n",
       " 'the',\n",
       " 'liberties',\n",
       " 'and',\n",
       " 'happiness',\n",
       " 'of',\n",
       " 'the',\n",
       " 'people',\n",
       " 'of',\n",
       " 'the',\n",
       " 'United',\n",
       " 'States',\n",
       " 'a',\n",
       " 'Government',\n",
       " 'instituted',\n",
       " 'by',\n",
       " 'themselves',\n",
       " 'for',\n",
       " 'these',\n",
       " 'essential',\n",
       " 'purposes',\n",
       " ',',\n",
       " 'and',\n",
       " 'may',\n",
       " 'enable',\n",
       " 'every',\n",
       " 'instrument',\n",
       " 'employed',\n",
       " 'in',\n",
       " 'its',\n",
       " 'administration',\n",
       " 'to',\n",
       " 'execute',\n",
       " 'with',\n",
       " 'success',\n",
       " 'the',\n",
       " 'functions',\n",
       " 'allotted',\n",
       " 'to',\n",
       " 'his',\n",
       " 'charge',\n",
       " '.',\n",
       " 'In',\n",
       " 'tendering',\n",
       " 'this',\n",
       " 'homage',\n",
       " 'to',\n",
       " 'the',\n",
       " 'Great',\n",
       " 'Author',\n",
       " 'of',\n",
       " 'every',\n",
       " 'public',\n",
       " 'and',\n",
       " 'private',\n",
       " 'good',\n",
       " ',',\n",
       " 'I',\n",
       " 'assure',\n",
       " 'myself',\n",
       " 'that',\n",
       " 'it',\n",
       " 'expresses',\n",
       " 'your',\n",
       " 'sentiments',\n",
       " 'not',\n",
       " 'less',\n",
       " 'than',\n",
       " 'my',\n",
       " 'own',\n",
       " ',',\n",
       " 'nor',\n",
       " 'those',\n",
       " 'of',\n",
       " 'my',\n",
       " 'fellow',\n",
       " 'citizens',\n",
       " 'at',\n",
       " 'large',\n",
       " 'less',\n",
       " 'than',\n",
       " 'either',\n",
       " '.',\n",
       " 'No',\n",
       " 'people',\n",
       " 'can',\n",
       " 'be',\n",
       " 'bound',\n",
       " 'to',\n",
       " 'acknowledge',\n",
       " 'and',\n",
       " 'adore',\n",
       " 'the',\n",
       " 'Invisible',\n",
       " 'Hand',\n",
       " 'which',\n",
       " 'conducts',\n",
       " 'the',\n",
       " 'affairs',\n",
       " 'of',\n",
       " 'men',\n",
       " 'more',\n",
       " 'than',\n",
       " 'those',\n",
       " 'of',\n",
       " 'the',\n",
       " 'United',\n",
       " 'States',\n",
       " '.',\n",
       " 'Every',\n",
       " 'step',\n",
       " 'by',\n",
       " 'which',\n",
       " 'they',\n",
       " 'have',\n",
       " 'advanced',\n",
       " 'to',\n",
       " 'the',\n",
       " 'character',\n",
       " 'of',\n",
       " 'an',\n",
       " 'independent',\n",
       " 'nation',\n",
       " 'seems',\n",
       " 'to',\n",
       " 'have',\n",
       " 'been',\n",
       " 'distinguished',\n",
       " 'by',\n",
       " 'some',\n",
       " 'token',\n",
       " 'of',\n",
       " 'providential',\n",
       " 'agency',\n",
       " ';',\n",
       " 'and',\n",
       " 'in',\n",
       " 'the',\n",
       " 'important',\n",
       " 'revolution',\n",
       " 'just',\n",
       " 'accomplished',\n",
       " 'in',\n",
       " 'the',\n",
       " 'system',\n",
       " 'of',\n",
       " 'their',\n",
       " 'united',\n",
       " 'government',\n",
       " 'the',\n",
       " 'tranquil',\n",
       " 'deliberations',\n",
       " 'and',\n",
       " 'voluntary',\n",
       " 'consent',\n",
       " 'of',\n",
       " 'so',\n",
       " 'many',\n",
       " 'distinct',\n",
       " 'communities',\n",
       " 'from',\n",
       " 'which',\n",
       " 'the',\n",
       " 'event',\n",
       " 'has',\n",
       " 'resulted',\n",
       " 'can',\n",
       " 'not',\n",
       " 'be',\n",
       " 'compared',\n",
       " 'with',\n",
       " 'the',\n",
       " 'means',\n",
       " 'by',\n",
       " 'which',\n",
       " 'most',\n",
       " 'governments',\n",
       " 'have',\n",
       " 'been',\n",
       " 'established',\n",
       " 'without',\n",
       " 'some',\n",
       " 'return',\n",
       " 'of',\n",
       " 'pious',\n",
       " 'gratitude',\n",
       " ',',\n",
       " 'along',\n",
       " 'with',\n",
       " 'an',\n",
       " 'humble',\n",
       " 'anticipation',\n",
       " 'of',\n",
       " 'the',\n",
       " 'future',\n",
       " 'blessings',\n",
       " 'which',\n",
       " 'the',\n",
       " 'past',\n",
       " 'seem',\n",
       " 'to',\n",
       " 'presage',\n",
       " '.',\n",
       " 'These',\n",
       " 'reflections',\n",
       " ',',\n",
       " 'arising',\n",
       " 'out',\n",
       " 'of',\n",
       " 'the',\n",
       " 'present',\n",
       " 'crisis',\n",
       " ',',\n",
       " 'have',\n",
       " 'forced',\n",
       " 'themselves',\n",
       " 'too',\n",
       " 'strongly',\n",
       " 'on',\n",
       " 'my',\n",
       " 'mind',\n",
       " 'to',\n",
       " 'be',\n",
       " 'suppressed',\n",
       " '.',\n",
       " 'You',\n",
       " 'will',\n",
       " 'join',\n",
       " 'with',\n",
       " 'me',\n",
       " ',',\n",
       " 'I',\n",
       " 'trust',\n",
       " ',',\n",
       " 'in',\n",
       " 'thinking',\n",
       " 'that',\n",
       " 'there',\n",
       " 'are',\n",
       " 'none',\n",
       " 'under',\n",
       " 'the',\n",
       " 'influence',\n",
       " 'of',\n",
       " 'which',\n",
       " 'the',\n",
       " 'proceedings',\n",
       " 'of',\n",
       " 'a',\n",
       " 'new',\n",
       " 'and',\n",
       " 'free',\n",
       " 'government',\n",
       " 'can',\n",
       " 'more',\n",
       " 'auspiciously',\n",
       " 'commence',\n",
       " '.',\n",
       " 'By',\n",
       " 'the',\n",
       " 'article',\n",
       " 'establishing',\n",
       " 'the',\n",
       " 'executive',\n",
       " 'department',\n",
       " 'it',\n",
       " 'is',\n",
       " 'made',\n",
       " 'the',\n",
       " 'duty',\n",
       " 'of',\n",
       " 'the',\n",
       " 'President',\n",
       " '``',\n",
       " 'to',\n",
       " 'recommend',\n",
       " 'to',\n",
       " 'your',\n",
       " 'consideration',\n",
       " 'such',\n",
       " 'measures',\n",
       " 'as',\n",
       " 'he',\n",
       " 'shall',\n",
       " 'judge',\n",
       " 'necessary',\n",
       " 'and',\n",
       " 'expedient',\n",
       " '.',\n",
       " \"''\",\n",
       " 'The',\n",
       " 'circumstances',\n",
       " 'under',\n",
       " 'which',\n",
       " 'I',\n",
       " 'now',\n",
       " 'meet',\n",
       " 'you',\n",
       " 'will',\n",
       " 'acquit',\n",
       " 'me',\n",
       " 'from',\n",
       " 'entering',\n",
       " 'into',\n",
       " 'that',\n",
       " 'subject',\n",
       " 'further',\n",
       " 'than',\n",
       " 'to',\n",
       " 'refer',\n",
       " 'to',\n",
       " 'the',\n",
       " 'great',\n",
       " 'constitutional',\n",
       " 'charter',\n",
       " 'under',\n",
       " 'which',\n",
       " 'you',\n",
       " 'are',\n",
       " 'assembled',\n",
       " ',',\n",
       " 'and',\n",
       " 'which',\n",
       " ',',\n",
       " 'in',\n",
       " 'defining',\n",
       " 'your',\n",
       " 'powers',\n",
       " ',',\n",
       " 'designates',\n",
       " 'the',\n",
       " 'objects',\n",
       " 'to',\n",
       " 'which',\n",
       " 'your',\n",
       " 'attention',\n",
       " 'is',\n",
       " 'to',\n",
       " 'be',\n",
       " 'given',\n",
       " '.',\n",
       " 'It',\n",
       " 'will',\n",
       " 'be',\n",
       " 'more',\n",
       " 'consistent',\n",
       " 'with',\n",
       " 'those',\n",
       " 'circumstances',\n",
       " ',',\n",
       " 'and',\n",
       " 'far',\n",
       " 'more',\n",
       " 'congenial',\n",
       " 'with',\n",
       " 'the',\n",
       " 'feelings',\n",
       " 'which',\n",
       " 'actuate',\n",
       " 'me',\n",
       " ',',\n",
       " 'to',\n",
       " 'substitute',\n",
       " ',',\n",
       " 'in',\n",
       " 'place',\n",
       " 'of',\n",
       " 'a',\n",
       " 'recommendation',\n",
       " 'of',\n",
       " 'particular',\n",
       " 'measures',\n",
       " ',',\n",
       " 'the',\n",
       " 'tribute',\n",
       " 'that',\n",
       " 'is',\n",
       " 'due',\n",
       " 'to',\n",
       " 'the',\n",
       " 'talents',\n",
       " ',',\n",
       " 'the',\n",
       " 'rectitude',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'patriotism',\n",
       " 'which',\n",
       " 'adorn',\n",
       " 'the',\n",
       " 'characters',\n",
       " 'selected',\n",
       " 'to',\n",
       " 'devise',\n",
       " 'and',\n",
       " 'adopt',\n",
       " 'them',\n",
       " '.',\n",
       " 'In',\n",
       " 'these',\n",
       " 'honorable',\n",
       " 'qualifications',\n",
       " 'I',\n",
       " 'behold',\n",
       " 'the',\n",
       " 'surest',\n",
       " 'pledges',\n",
       " 'that',\n",
       " 'as',\n",
       " 'on',\n",
       " 'one',\n",
       " 'side',\n",
       " 'no',\n",
       " 'local',\n",
       " 'prejudices',\n",
       " 'or',\n",
       " 'attachments',\n",
       " ',',\n",
       " 'no',\n",
       " 'separate',\n",
       " 'views',\n",
       " 'nor',\n",
       " 'party',\n",
       " 'animosities',\n",
       " ',',\n",
       " 'will',\n",
       " 'misdirect',\n",
       " 'the',\n",
       " 'comprehensive',\n",
       " 'and',\n",
       " 'equal',\n",
       " 'eye',\n",
       " 'which',\n",
       " 'ought',\n",
       " 'to',\n",
       " 'watch',\n",
       " 'over',\n",
       " 'this',\n",
       " 'great',\n",
       " 'assemblage',\n",
       " 'of',\n",
       " 'communities',\n",
       " 'and',\n",
       " 'interests',\n",
       " ',',\n",
       " 'so',\n",
       " ',',\n",
       " 'on',\n",
       " 'another',\n",
       " ',',\n",
       " 'that',\n",
       " 'the',\n",
       " 'foundation',\n",
       " 'of',\n",
       " 'our',\n",
       " 'national',\n",
       " 'policy',\n",
       " 'will',\n",
       " 'be',\n",
       " 'laid',\n",
       " 'in',\n",
       " 'the',\n",
       " 'pure',\n",
       " 'and',\n",
       " 'immutable',\n",
       " 'principles',\n",
       " 'of',\n",
       " 'private',\n",
       " 'morality',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'preeminence',\n",
       " 'of',\n",
       " 'free',\n",
       " 'government',\n",
       " 'be',\n",
       " 'exemplified',\n",
       " 'by',\n",
       " 'all',\n",
       " 'the',\n",
       " 'attributes',\n",
       " 'which',\n",
       " 'can',\n",
       " 'win',\n",
       " 'the',\n",
       " 'affections',\n",
       " 'of',\n",
       " 'its',\n",
       " 'citizens',\n",
       " 'and',\n",
       " 'command',\n",
       " 'the',\n",
       " 'respect',\n",
       " 'of',\n",
       " 'the',\n",
       " 'world',\n",
       " '.',\n",
       " 'I',\n",
       " 'dwell',\n",
       " 'on',\n",
       " 'this',\n",
       " 'prospect',\n",
       " 'with',\n",
       " 'every',\n",
       " 'satisfaction',\n",
       " 'which',\n",
       " 'an',\n",
       " 'ardent',\n",
       " 'love',\n",
       " 'for',\n",
       " 'my',\n",
       " 'country',\n",
       " 'can',\n",
       " 'inspire',\n",
       " ',',\n",
       " 'since',\n",
       " 'there',\n",
       " 'is',\n",
       " 'no',\n",
       " 'truth',\n",
       " 'more',\n",
       " 'thoroughly',\n",
       " 'established',\n",
       " 'than',\n",
       " 'that',\n",
       " 'there',\n",
       " 'exists',\n",
       " 'in',\n",
       " 'the',\n",
       " 'economy',\n",
       " 'and',\n",
       " 'course',\n",
       " 'of',\n",
       " 'nature',\n",
       " 'an',\n",
       " 'indissoluble',\n",
       " 'union',\n",
       " 'between',\n",
       " 'virtue',\n",
       " 'and',\n",
       " 'happiness',\n",
       " ';',\n",
       " 'between',\n",
       " 'duty',\n",
       " 'and',\n",
       " 'advantage',\n",
       " ';',\n",
       " 'between',\n",
       " 'the',\n",
       " 'genuine',\n",
       " 'maxims',\n",
       " 'of',\n",
       " 'an',\n",
       " 'honest',\n",
       " 'and',\n",
       " 'magnanimous',\n",
       " 'policy',\n",
       " 'and',\n",
       " 'the',\n",
       " 'solid',\n",
       " 'rewards',\n",
       " 'of',\n",
       " 'public',\n",
       " 'prosperity',\n",
       " 'and',\n",
       " 'felicity',\n",
       " ';',\n",
       " 'since',\n",
       " 'we',\n",
       " 'ought',\n",
       " 'to',\n",
       " 'be',\n",
       " 'no',\n",
       " 'less',\n",
       " ...]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spch_tokens = word_tokenize(spch)\n",
    "spch_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spch_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 70, ',': 48, 'of': 47, 'and': 35, 'to': 33, 'which': 23, 'in': 20, 'my': 16, 'by': 15, '.': 15, ...})"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for word in spch_tokens:\n",
    "    fdist[word.lower()]+=1\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "432"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of distinct tokens\n",
    "len(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 70),\n",
       " (',', 48),\n",
       " ('of', 47),\n",
       " ('and', 35),\n",
       " ('to', 33),\n",
       " ('which', 23),\n",
       " ('in', 20),\n",
       " ('my', 16),\n",
       " ('by', 15),\n",
       " ('.', 15)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 most common words\n",
    "fdist_top10 = fdist.most_common(10)\n",
    "fdist_top10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use blankline tokenization which will tell us the number of paragraphs that are seperate dby a new line\n",
    "from nltk.tokenize import blankline_tokenize\n",
    "spch_blank = blankline_tokenize(spch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ngrams, Bigrams, Trigrams"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from nltk.util import bigrams, trigrams, ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fellow',\n",
       " '-',\n",
       " 'Citizens',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Senate',\n",
       " 'and',\n",
       " 'of',\n",
       " 'the',\n",
       " 'House',\n",
       " 'of',\n",
       " 'Representatives',\n",
       " ':',\n",
       " 'Among',\n",
       " 'the',\n",
       " 'vicissitudes',\n",
       " 'inciden']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_string = spch[:100]\n",
    "quotes_tokens = nltk.word_tokenize(my_string)\n",
    "quotes_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Fellow', '-'),\n",
       " ('-', 'Citizens'),\n",
       " ('Citizens', 'of'),\n",
       " ('of', 'the'),\n",
       " ('the', 'Senate'),\n",
       " ('Senate', 'and'),\n",
       " ('and', 'of'),\n",
       " ('of', 'the'),\n",
       " ('the', 'House'),\n",
       " ('House', 'of'),\n",
       " ('of', 'Representatives'),\n",
       " ('Representatives', ':'),\n",
       " (':', 'Among'),\n",
       " ('Among', 'the'),\n",
       " ('the', 'vicissitudes'),\n",
       " ('vicissitudes', 'inciden')]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_bigrams = list(nltk.bigrams(quotes_tokens))\n",
    "quotes_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Fellow', '-', 'Citizens'),\n",
       " ('-', 'Citizens', 'of'),\n",
       " ('Citizens', 'of', 'the'),\n",
       " ('of', 'the', 'Senate'),\n",
       " ('the', 'Senate', 'and'),\n",
       " ('Senate', 'and', 'of'),\n",
       " ('and', 'of', 'the'),\n",
       " ('of', 'the', 'House'),\n",
       " ('the', 'House', 'of'),\n",
       " ('House', 'of', 'Representatives'),\n",
       " ('of', 'Representatives', ':'),\n",
       " ('Representatives', ':', 'Among'),\n",
       " (':', 'Among', 'the'),\n",
       " ('Among', 'the', 'vicissitudes'),\n",
       " ('the', 'vicissitudes', 'inciden')]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_trigrams = list(nltk.trigrams(quotes_tokens))\n",
    "quotes_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Fellow', '-', 'Citizens', 'of', 'the'),\n",
       " ('-', 'Citizens', 'of', 'the', 'Senate'),\n",
       " ('Citizens', 'of', 'the', 'Senate', 'and'),\n",
       " ('of', 'the', 'Senate', 'and', 'of'),\n",
       " ('the', 'Senate', 'and', 'of', 'the'),\n",
       " ('Senate', 'and', 'of', 'the', 'House'),\n",
       " ('and', 'of', 'the', 'House', 'of'),\n",
       " ('of', 'the', 'House', 'of', 'Representatives'),\n",
       " ('the', 'House', 'of', 'Representatives', ':'),\n",
       " ('House', 'of', 'Representatives', ':', 'Among'),\n",
       " ('of', 'Representatives', ':', 'Among', 'the'),\n",
       " ('Representatives', ':', 'Among', 'the', 'vicissitudes'),\n",
       " (':', 'Among', 'the', 'vicissitudes', 'inciden')]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_ngrams = list(nltk.ngrams(quotes_tokens, 5))\n",
    "quotes_ngrams"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "pst=PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem('having')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give:give\n",
      "giving:give\n",
      "given:given\n",
      "gave:gave\n"
     ]
    }
   ],
   "source": [
    "words_to_stem=['give','giving','given','gave']\n",
    "for w in words_to_stem:\n",
    "    print(w + ':' + pst.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lst=LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give:giv\n",
      "giving:giv\n",
      "given:giv\n",
      "gave:gav\n"
     ]
    }
   ],
   "source": [
    "words_to_stem=['give','giving','given','gave']\n",
    "for w in words_to_stem:\n",
    "    print(w + ':' + lst.stem(w))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lancaster Stemmer is a more aggressive stemmer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "sbst=SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give:give\n",
      "giving:give\n",
      "given:given\n",
      "gave:gave\n"
     ]
    }
   ],
   "source": [
    "words_to_stem=['give','giving','given','gave']\n",
    "for w in words_to_stem:\n",
    "    print(w + ':' + sbst.stem(w))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to remember that **Stemming** does not always work correctly and give us the desired result. e.g. fish, fishing and fishes will all stem to fish. **Lemmatization** can be used as an alternative process for discoverign the root meaning of the word."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "word_lem=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'corpus'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lem.lemmatize('corpora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give:give\n",
      "giving:giving\n",
      "given:given\n",
      "gave:gave\n"
     ]
    }
   ],
   "source": [
    "words_to_stem=['give','giving','given','gave']\n",
    "for w in words_to_stem:\n",
    "    print(w + ':' + word_lem.lemmatize(w))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')[:20]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets re-check the fdist top 10 to see the tokens with the highest number of frequency, but with punctuation removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 70),\n",
       " (',', 48),\n",
       " ('of', 47),\n",
       " ('and', 35),\n",
       " ('to', 33),\n",
       " ('which', 23),\n",
       " ('in', 20),\n",
       " ('my', 16),\n",
       " ('by', 15),\n",
       " ('.', 15)]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# use regex to remove punctuation\n",
    "punctuation = re.compile(r'[-.?!,:;()|0-9]')\n",
    "# use regex to remove spaces\n",
    "spaces = r\"\\s+\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_punctuation=[]\n",
    "for words in spch_tokens:\n",
    "    word=punctuation.sub(\"\",words)\n",
    "    if len(word)>0:\n",
    "        post_punctuation.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fellow',\n",
       " 'Citizens',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Senate',\n",
       " 'and',\n",
       " 'of',\n",
       " 'the',\n",
       " 'House',\n",
       " 'of',\n",
       " 'Representatives',\n",
       " 'Among',\n",
       " 'the',\n",
       " 'vicissitudes',\n",
       " 'incident']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_punctuation[:15]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POS: Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Kasheme', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('natural', 'JJ'),\n",
       " ('when', 'WRB'),\n",
       " ('it', 'PRP'),\n",
       " ('comes', 'VBZ'),\n",
       " ('to', 'TO'),\n",
       " ('drawing', 'VBG')]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"Kasheme is a natural when it comes to drawing\"\n",
    "sent_tokens = word_tokenize(sent)\n",
    "nltk.pos_tag(sent_tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NER: Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  (ORGANIZATION US/NNP)\n",
      "  President/NNP\n",
      "  stays/VBZ\n",
      "  in/IN\n",
      "  the/DT\n",
      "  (FACILITY White/NNP House/NNP))\n"
     ]
    }
   ],
   "source": [
    "ne_sent = \"The US President stays in the White House\"\n",
    "ne_tokens = word_tokenize(ne_sent)\n",
    "ne_tags = nltk.pos_tag(ne_tokens)\n",
    "ne_ner = ne_chunk(ne_tags)\n",
    "print(ne_ner)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('big', 'JJ'),\n",
       " ('cat', 'NN'),\n",
       " ('ate', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('little', 'JJ'),\n",
       " ('mouse', 'NN'),\n",
       " ('who', 'WP'),\n",
       " ('was', 'VBD'),\n",
       " ('after', 'IN'),\n",
       " ('fresh', 'JJ'),\n",
       " ('cheese', 'NN')]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new = \"The big cat ate the little mouse who was after fresh cheese\"\n",
    "new_tokens = nltk.pos_tag(word_tokenize(new))\n",
    "new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_np = r\"NP: {<DT>?<JJ>*<NN>}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_parser = nltk.RegexpParser(grammar_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'svgling'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\UKE12399093\\Anaconda3\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\UKE12399093\\Anaconda3\\lib\\site-packages\\nltk\\tree\\tree.py\u001b[0m in \u001b[0;36m_repr_svg_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_repr_svg_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 783\u001b[1;33m         \u001b[1;32mfrom\u001b[0m \u001b[0msvgling\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdraw_tree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    784\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdraw_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_repr_svg_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'svgling'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tree('S', [Tree('NP', [('The', 'DT'), ('big', 'JJ'), ('cat', 'NN')]), ('ate', 'VBD'), Tree('NP', [('the', 'DT'), ('little', 'JJ'), ('mouse', 'NN')]), ('who', 'WP'), ('was', 'VBD'), ('after', 'IN'), Tree('NP', [('fresh', 'JJ'), ('cheese', 'NN')])])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_result = chunk_parser.parse(new_tokens)\n",
    "chunk_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
