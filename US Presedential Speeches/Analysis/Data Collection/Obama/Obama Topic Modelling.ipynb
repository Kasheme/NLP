{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obama Topic Modelling - *Top2Vec*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we'll perform the topic modelling for Obama's speeches. We'll be using a technique called <span style=\"color:red; background-color:grey\">**Top2Vec**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BO_Nov2006_df = pd.read_pickle('Obama Speeches Raw/BO_Nov2006_df.pkl')\n",
    "BO_Feb2009_df = pd.read_pickle('Obama Speeches Raw/BO_Feb2009_df.pkl')\n",
    "BO_May2011_df = pd.read_pickle('Obama Speeches Raw/BO_May2011_df.pkl')\n",
    "BO_Aug2014_df = pd.read_pickle('Obama Speeches Raw/BO_Aug2014_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define function for converting df to list of strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Top2Vec* model requires a list as input so we'll convert each df to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_list(speech_df):\n",
    "    dflist = speech_df['sentence'].astype(str).tolist()\n",
    "    newlist = []\n",
    "\n",
    "    for text in dflist:\n",
    "        newlist.append(text)\n",
    "    return newlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BO_Nov2006_list = df_to_list(BO_Nov2006_df)\n",
    "BO_Feb2009_list = df_to_list(BO_Feb2009_df)\n",
    "BO_May2011_list = df_to_list(BO_May2011_df)\n",
    "BO_Aug2014_list = df_to_list(BO_Aug2014_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Throughout American history,there have been moments that call on us to meet the challenges ofan uncertain world, and pay whatever price is required to secureour freedom.',\n",
       " 'They are the soul-trying times our forbearers spokeof, when the ease of complacency and self-interest must give wayto the more difficult task of rendering judgment on what is bestfor the nation and for posterity, and then acting on that judgment?',\n",
       " 'making the hard choices and sacrifices necessary to uphold ourmost deeply held values and ideals.',\n",
       " 'This was true for thosewho went to Lexington and Concord.',\n",
       " 'It was true for those who lieburied at Gettysburg.',\n",
       " 'It was true for those who built democracyâ€™sarsenal to vanquish fascism, and who then built a series of alliancesand a world order that would ultimately defeat communism.',\n",
       " 'And this has been truefor those of us who looked on the rubble and ashes of 9/11, andmade a solemn pledge that such an atrocity would never again happenon United States soil; that we would do whatever it took to huntdown those responsible, and use every tool at our disposal ?',\n",
       " 'diplomatic,economic, and military ?',\n",
       " 'to root out both the agents of terrorismand the conditions that helped breed it.',\n",
       " 'In each case, what hasbeen required to meet the challenges we face has been good judgmentand clear vision from our leaders, and a fundamental seriousnessand engagement on the part of the American people ?']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BO_Nov2006_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(BO_Nov2006_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Top2Vec Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 182] The operating system cannot run %1. Error loading \"c:\\Users\\User\\miniconda3\\envs\\snowflakes\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[0;32m      5\u001b[0m warnings\u001b[39m.\u001b[39mwarn \u001b[39m=\u001b[39m warn\n\u001b[1;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtop2vec\u001b[39;00m \u001b[39mimport\u001b[39;00m Top2Vec\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\snowflakes\\lib\\site-packages\\top2vec\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtop2vec\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mTop2Vec\u001b[39;00m \u001b[39mimport\u001b[39;00m Top2Vec\n\u001b[0;32m      3\u001b[0m __version__ \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m1.0.26\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\snowflakes\\lib\\site-packages\\top2vec\\Top2Vec.py:38\u001b[0m\n\u001b[0;32m     35\u001b[0m     _HAVE_TENSORFLOW \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 38\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39msentence_transformers\u001b[39;00m \u001b[39mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m     40\u001b[0m     _HAVE_TORCH \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\snowflakes\\lib\\site-packages\\sentence_transformers\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m __version__ \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m2.2.2\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m __MODEL_HUB_ORGANIZATION__ \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msentence-transformers\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m SentencesDataset, ParallelSentencesDataset\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mLoggingHandler\u001b[39;00m \u001b[39mimport\u001b[39;00m LoggingHandler\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mSentenceTransformer\u001b[39;00m \u001b[39mimport\u001b[39;00m SentenceTransformer\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\snowflakes\\lib\\site-packages\\sentence_transformers\\datasets\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mDenoisingAutoEncoderDataset\u001b[39;00m \u001b[39mimport\u001b[39;00m DenoisingAutoEncoderDataset\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mNoDuplicatesDataLoader\u001b[39;00m \u001b[39mimport\u001b[39;00m NoDuplicatesDataLoader\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mParallelSentencesDataset\u001b[39;00m \u001b[39mimport\u001b[39;00m ParallelSentencesDataset\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\snowflakes\\lib\\site-packages\\sentence_transformers\\datasets\\DenoisingAutoEncoderDataset.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m List\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mreaders\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mInputExample\u001b[39;00m \u001b[39mimport\u001b[39;00m InputExample\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\snowflakes\\lib\\site-packages\\torch\\__init__.py:122\u001b[0m\n\u001b[0;32m    120\u001b[0m     err \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mWinError(last_error)\n\u001b[0;32m    121\u001b[0m     err\u001b[39m.\u001b[39mstrerror \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m Error loading \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mdll\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m or one of its dependencies.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 122\u001b[0m     \u001b[39mraise\u001b[39;00m err\n\u001b[0;32m    123\u001b[0m \u001b[39melif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     is_loaded \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 182] The operating system cannot run %1. Error loading \"c:\\Users\\User\\miniconda3\\envs\\snowflakes\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "# suppress warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "from top2vec import Top2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mprint\u001b[39m(tf\u001b[39m.\u001b[39;49m__version__)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute '__version__'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "universal-sentence-encoder is not available.\n\nTry: pip install top2vec[sentence_encoders]\n\nAlternatively try: pip install tensorflow tensorflow_hub tensorflow_text",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m Top2Vec(documents\u001b[39m=\u001b[39;49mBO_Nov2006_list, embedding_model\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39muniversal-sentence-encoder\u001b[39;49m\u001b[39m'\u001b[39;49m, speed\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdeep-learn\u001b[39;49m\u001b[39m'\u001b[39;49m, workers\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\snowflakes\\lib\\site-packages\\top2vec\\Top2Vec.py:312\u001b[0m, in \u001b[0;36mTop2Vec.__init__\u001b[1;34m(self, documents, min_count, embedding_model, embedding_model_path, speed, use_corpus_file, document_ids, keep_documents, workers, tokenizer, use_embedding_model_tokenizer, umap_args, hdbscan_args, verbose)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_model \u001b[39m=\u001b[39m embedding_model\n\u001b[1;32m--> 312\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_import_status()\n\u001b[0;32m    314\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39mPre-processing documents for training\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    316\u001b[0m \u001b[39m# preprocess documents\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\miniconda3\\envs\\snowflakes\\lib\\site-packages\\top2vec\\Top2Vec.py:824\u001b[0m, in \u001b[0;36mTop2Vec._check_import_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    822\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_model \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdistiluse-base-multilingual-cased\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    823\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _HAVE_TENSORFLOW:\n\u001b[1;32m--> 824\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_model\u001b[39m}\u001b[39;00m\u001b[39m is not available.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    825\u001b[0m                           \u001b[39m\"\u001b[39m\u001b[39mTry: pip install top2vec[sentence_encoders]\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    826\u001b[0m                           \u001b[39m\"\u001b[39m\u001b[39mAlternatively try: pip install tensorflow tensorflow_hub tensorflow_text\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    827\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _HAVE_TORCH:\n",
      "\u001b[1;31mImportError\u001b[0m: universal-sentence-encoder is not available.\n\nTry: pip install top2vec[sentence_encoders]\n\nAlternatively try: pip install tensorflow tensorflow_hub tensorflow_text"
     ]
    }
   ],
   "source": [
    "model = Top2Vec(documents=BO_Nov2006_list, \n",
    "# embedding_model='universal-sentence-encoder', speed='deep-learn', workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mnumber of topics  model has retrieved\n",
    "model.get_num_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100  99]\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "topic_sizes, topic_nums = model.get_topic_sizes()\n",
    "print(topic_sizes)\n",
    "print(topic_nums)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowflakes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
